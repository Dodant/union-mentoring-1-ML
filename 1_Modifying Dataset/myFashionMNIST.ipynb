{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5498312-59f6-4d30-a3a7-d8f74397e216",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.datasets.utils import download_and_extract_archive, extract_archive, verify_str_arg, check_integrity\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from urllib.error import URLError\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import codecs\n",
    "from PIL import Image\n",
    "\n",
    "class myMNIST(VisionDataset):\n",
    "\n",
    "    mirrors = [\n",
    "        'http://yann.lecun.com/exdb/mnist/',\n",
    "        'https://ossci-datasets.s3.amazonaws.com/mnist/',\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"train-images-idx3-ubyte.gz\", \"f68b3c2dcbeaaa9fbdd348bbdeb94873\"),\n",
    "        (\"train-labels-idx1-ubyte.gz\", \"d53e105ee54ea40749a09fcbcd1e9432\"),\n",
    "        (\"t10k-images-idx3-ubyte.gz\", \"9fb629c4189551a2d022fa330f9573f3\"),\n",
    "        (\"t10k-labels-idx1-ubyte.gz\", \"ec29112dd5afa0611ce80d1b7f02629c\")\n",
    "    ]\n",
    "\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
    "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "\n",
    "    @property\n",
    "    def train_labels(self):\n",
    "        warnings.warn(\"train_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def test_labels(self):\n",
    "        warnings.warn(\"test_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        warnings.warn(\"train_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        warnings.warn(\"test_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False,\n",
    "    ) -> None:\n",
    "        super(myMNIST, self).__init__(root, transform=transform, target_transform=target_transform)\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if self._check_legacy_exist():\n",
    "            self.data, self.targets = self._load_legacy_data()\n",
    "            return\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        self.data, self.targets = self._load_data()\n",
    "        \n",
    "        import random \n",
    "        imbalance_list = random.sample(range(0,10), 5)\n",
    "\n",
    "        new_data = []\n",
    "        new_target = []\n",
    "\n",
    "        for i in range(len(self.targets)):\n",
    "            if self.targets[i] in imbalance_list:\n",
    "                if random.random() >= 0.9:\n",
    "                    new_data.append(self.data[i])\n",
    "                    new_target.append(self.targets[i])\n",
    "            else:\n",
    "                new_data.append(self.data[i])\n",
    "                new_target.append(self.targets[i])\n",
    "\n",
    "        self.data = new_data\n",
    "        self.targets = new_target\n",
    "\n",
    "\n",
    "    def _check_legacy_exist(self):\n",
    "        processed_folder_exists = os.path.exists(self.processed_folder)\n",
    "        if not processed_folder_exists:\n",
    "            return False\n",
    "\n",
    "        return all(\n",
    "            check_integrity(os.path.join(self.processed_folder, file)) for file in (self.training_file, self.test_file)\n",
    "        )\n",
    "\n",
    "    def _load_legacy_data(self):\n",
    "        # This is for BC only. We no longer cache the data in a custom binary, but simply read from the raw data\n",
    "        # directly.\n",
    "        data_file = self.training_file if self.train else self.test_file\n",
    "        return torch.load(os.path.join(self.processed_folder, data_file))\n",
    "\n",
    "    def _load_data(self):\n",
    "        image_file = f\"{'train' if self.train else 't10k'}-images-idx3-ubyte\"\n",
    "        data = read_image_file(os.path.join(self.raw_folder, image_file))\n",
    "\n",
    "        label_file = f\"{'train' if self.train else 't10k'}-labels-idx1-ubyte\"\n",
    "        targets = read_label_file(os.path.join(self.raw_folder, label_file))\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self) -> str:\n",
    "        return os.path.join(self.root, self.__class__.__name__, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_folder(self) -> str:\n",
    "        return os.path.join(self.root, self.__class__.__name__, 'processed')\n",
    "\n",
    "    @property\n",
    "    def class_to_idx(self) -> Dict[str, int]:\n",
    "        return {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        return all(\n",
    "            check_integrity(os.path.join(self.raw_folder, os.path.splitext(os.path.basename(url))[0]))\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "\n",
    "    def download(self) -> None:\n",
    "        \"\"\"Download the MNIST data if it doesn't exist already.\"\"\"\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        os.makedirs(self.raw_folder, exist_ok=True)\n",
    "\n",
    "        # download files\n",
    "        for filename, md5 in self.resources:\n",
    "            for mirror in self.mirrors:\n",
    "                url = \"{}{}\".format(mirror, filename)\n",
    "                try:\n",
    "                    print(\"Downloading {}\".format(url))\n",
    "                    download_and_extract_archive(\n",
    "                        url, download_root=self.raw_folder,\n",
    "                        filename=filename,\n",
    "                        md5=md5\n",
    "                    )\n",
    "                except URLError as error:\n",
    "                    print(\n",
    "                        \"Failed to download (trying next):\\n{}\".format(error)\n",
    "                    )\n",
    "                    continue\n",
    "                finally:\n",
    "                    print()\n",
    "                break\n",
    "            else:\n",
    "                raise RuntimeError(\"Error downloading {}\".format(filename))\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")\n",
    "    \n",
    "\n",
    "class myFashionMNIST(myMNIST):\n",
    "    mirrors = [\n",
    "        \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"train-images-idx3-ubyte.gz\", \"8d4fb7e6c68d591d4c3dfef9ec88bf0d\"),\n",
    "        (\"train-labels-idx1-ubyte.gz\", \"25c81989df183df01b3e8a0aad5dffbe\"),\n",
    "        (\"t10k-images-idx3-ubyte.gz\", \"bef4ecab320f06d8554ea6380940ec79\"),\n",
    "        (\"t10k-labels-idx1-ubyte.gz\", \"bb300cfdad3c16e7a12a480ee83cd310\")\n",
    "    ]\n",
    "    classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "def get_int(b: bytes) -> int:\n",
    "    return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "    \n",
    "SN3_PASCALVINCENT_TYPEMAP = {\n",
    "    8: (torch.uint8, np.uint8, np.uint8),\n",
    "    9: (torch.int8, np.int8, np.int8),\n",
    "    11: (torch.int16, np.dtype('>i2'), 'i2'),\n",
    "    12: (torch.int32, np.dtype('>i4'), 'i4'),\n",
    "    13: (torch.float32, np.dtype('>f4'), 'f4'),\n",
    "    14: (torch.float64, np.dtype('>f8'), 'f8')\n",
    "}\n",
    "\n",
    "    \n",
    "def read_image_file(path: str) -> torch.Tensor:\n",
    "    x = read_sn3_pascalvincent_tensor(path, strict=False)\n",
    "    assert(x.dtype == torch.uint8)\n",
    "    assert(x.ndimension() == 3)\n",
    "    return x\n",
    "\n",
    "\n",
    "def read_label_file(path: str) -> torch.Tensor:\n",
    "    x = read_sn3_pascalvincent_tensor(path, strict=False)\n",
    "    assert(x.dtype == torch.uint8)\n",
    "    assert(x.ndimension() == 1)\n",
    "    return x.long()\n",
    "\n",
    "\n",
    "def read_sn3_pascalvincent_tensor(path: str, strict: bool = True) -> torch.Tensor:\n",
    "    \"\"\"Read a SN3 file in \"Pascal Vincent\" format (Lush file 'libidx/idx-io.lsh').\n",
    "       Argument may be a filename, compressed filename, or file object.\n",
    "    \"\"\"\n",
    "    # read\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    # parse\n",
    "    magic = get_int(data[0:4])\n",
    "    nd = magic % 256\n",
    "    ty = magic // 256\n",
    "    assert 1 <= nd <= 3\n",
    "    assert 8 <= ty <= 14\n",
    "    m = SN3_PASCALVINCENT_TYPEMAP[ty]\n",
    "    s = [get_int(data[4 * (i + 1): 4 * (i + 2)]) for i in range(nd)]\n",
    "    parsed = np.frombuffer(data, dtype=m[1], offset=(4 * (nd + 1)))\n",
    "    assert parsed.shape[0] == np.prod(s) or not strict\n",
    "    return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9298f353-b628-4013-9006-19ef509c18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize((64, 64))])\n",
    "\n",
    "# mymnist_trainset = myMNIST(root='./data1', train=True, download=True)\n",
    "# mymnist_trainloader = torch.utils.data.DataLoader(mymnist_trainset, batch_size=batch_size, shuffle=True, num_workers=2 ,pin_memory=True)\n",
    "\n",
    "myfmnist_trainset = myFashionMNIST(root='./data1', train=True, download=True, transform=transform)\n",
    "myfmnist_trainloader = torch.utils.data.DataLoader(myfmnist_trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2110843e-f660-4916-8e46-245c2598e6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32961\n"
     ]
    }
   ],
   "source": [
    "# print(len(mymnist_trainset.data))\n",
    "print(len(myfmnist_trainset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfba4bce-2844-47a7-8514-3899543e9840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6000, 6000, 618, 6000, 6000, 592, 6000, 596, 534, 621]\n"
     ]
    }
   ],
   "source": [
    "# num_of_class_MNIST = [0,0,0,0,0,0,0,0,0,0]\n",
    "num_of_class_fMNIST = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "# for i in mymnist_trainset.targets:\n",
    "#     num_of_class_MNIST[i] += 1\n",
    "for i in myfmnist_trainset.targets:\n",
    "    num_of_class_fMNIST[i] += 1\n",
    "    \n",
    "# print(num_of_class_MNIST)\n",
    "print(num_of_class_fMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79c6aea3-c3a2-4378-b85c-4f1966dfa5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mymnist_testset = torchvision.datasets.MNIST(root='./data1', train=False, download=True)\n",
    "# mymnist_testloader = torch.utils.data.DataLoader(mymnist_testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "myfmnist_testset = torchvision.datasets.FashionMNIST(root='./data1', train=False, download=True, transform=transform)\n",
    "myfmnist_testloader = torch.utils.data.DataLoader(myfmnist_testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8d7f0df-15be-43c1-82f0-71ae29225347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc5): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 128, 3)\n",
    "        self.conv6 = nn.Conv2d(128, 128, 3)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 128)\n",
    "        self.fc5 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1) # 배치를 제외한 모든 차원을 평탄화(flatten)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2317e4f2-59ed-46f1-acc3-f1797d65ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "def FL(predict, label, alpha=.25, gamma=2.0):\n",
    "    predict_sm = torch.softmax(predict, dim=1)\n",
    "    label = F.one_hot(label, num_classes=len(predict[0]))\n",
    "    factor = alpha * (1-predict_sm)**gamma\n",
    "    return torch.mean((factor * (label * -torch.log(predict_sm))).sum(dim=1))\n",
    "# def CEE(predict, label):\n",
    "#     delta = 1e-7\n",
    "#     predict_sm = torch.softmax(predict, dim=1)\n",
    "#     label = F.one_hot(label, num_classes=len(predict[0]))\n",
    "#     return torch.mean((label * -torch.log(predict_sm + delta)).sum(dim=1))\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af975de-d3f4-46ce-a8a7-bf18a2671a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history, train_acc_history = [], []\n",
    "# valid_loss_history, valid_acc_history = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24cfee6e-7323-45b1-b21b-ebb9b7f7ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 || tl: 0.249 | ta: 39.434\n",
      "epoch: 2 || tl: 0.090 | ta: 72.637\n",
      "epoch: 3 || tl: 0.074 | ta: 75.526\n",
      "epoch: 4 || tl: 0.066 | ta: 77.583\n",
      "epoch: 5 || tl: 0.061 | ta: 78.948\n",
      "epoch: 6 || tl: 0.057 | ta: 80.295\n",
      "epoch: 7 || tl: 0.052 | ta: 81.454\n",
      "epoch: 8 || tl: 0.048 | ta: 82.616\n",
      "epoch: 9 || tl: 0.045 | ta: 83.650\n",
      "epoch: 10 || tl: 0.042 | ta: 84.694\n",
      "epoch: 11 || tl: 0.040 | ta: 85.161\n",
      "epoch: 12 || tl: 0.037 | ta: 86.208\n",
      "epoch: 13 || tl: 0.035 | ta: 86.593\n",
      "epoch: 14 || tl: 0.034 | ta: 87.000\n",
      "epoch: 15 || tl: 0.032 | ta: 87.461\n",
      "epoch: 16 || tl: 0.030 | ta: 88.074\n",
      "epoch: 17 || tl: 0.029 | ta: 88.495\n",
      "epoch: 18 || tl: 0.027 | ta: 88.917\n",
      "epoch: 19 || tl: 0.025 | ta: 89.303\n",
      "epoch: 20 || tl: 0.024 | ta: 89.733\n",
      "epoch: 21 || tl: 0.023 | ta: 89.982\n",
      "epoch: 22 || tl: 0.021 | ta: 90.613\n",
      "epoch: 23 || tl: 0.020 | ta: 90.880\n",
      "epoch: 24 || tl: 0.019 | ta: 91.193\n",
      "epoch: 25 || tl: 0.018 | ta: 91.630\n",
      "epoch: 26 || tl: 0.017 | ta: 92.066\n",
      "epoch: 27 || tl: 0.016 | ta: 92.327\n",
      "epoch: 28 || tl: 0.015 | ta: 92.840\n",
      "epoch: 29 || tl: 0.014 | ta: 92.807\n",
      "epoch: 30 || tl: 0.013 | ta: 93.298\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):   # 데이터셋을 수차례 반복합니다.\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    \n",
    "    train_samples = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    for inputs, labels in myfmnist_trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만들고\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화를 한 후\n",
    "        outputs = net(inputs)\n",
    "        loss = FL(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += torch.sum(preds == labels.data)\n",
    "        train_samples += len(inputs)\n",
    "    \n",
    "#     else:\n",
    "#         # 훈련팔 필요가 없으므로 메모리 절약\n",
    "#         with torch.no_grad():\n",
    "#             for valid_input, valid_label in valid_loader:\n",
    "#                 valid_input, valid_label = valid_input.to(device), valid_label.to(device)\n",
    "#                 valid_outputs = net(valid_input)\n",
    "#                 valid_loss = criterion(valid_outputs, valid_label)\n",
    "\n",
    "#                 _, valid_preds = torch.max(valid_outputs, 1)\n",
    "#                 valid_loss += valid_loss.item()\n",
    "#                 valid_acc += torch.sum(valid_preds == valid_label.data)\n",
    "#                 valid_samples += len(valid_input)\n",
    "                \n",
    "    epoch_loss = train_loss / len(myfmnist_trainloader)\n",
    "    epoch_acc = train_acc.float() / train_samples * 100\n",
    "    train_loss_history.append(epoch_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "\n",
    "#     valid_epoch_loss = valid_loss * 10 / len(valid_loader)\n",
    "#     valid_epoch_acc = valid_acc.float() / valid_samples * 100\n",
    "#     valid_loss_history.append(valid_epoch_loss)\n",
    "#     valid_acc_history.append(valid_epoch_acc)\n",
    "\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "    print(f\"epoch: {epoch + 1} || tl: {epoch_loss:.3f} | ta: {epoch_acc:.3f}\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2ad3cd5-b785-448f-8fa1-3eb8d155520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for images, labels in myfmnist_testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57ca8a2d-e6f4-403b-84f8-980f6014e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.8 97.8 69.1 92.5 89.9 94.4 69.2 93.6 92.4 90.3 "
     ]
    }
   ],
   "source": [
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "               'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for images, labels in myfmnist_testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(accuracy, end=' ')\n",
    "#     print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dce6010-b7ea-4791-ad9a-7acf74ee199e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[577, 627, 6000, 606, 6000, 589, 6000, 6000, 6000, 579]\n"
     ]
    }
   ],
   "source": [
    "# num_of_class_MNIST = [0,0,0,0,0,0,0,0,0,0]\n",
    "num_of_class_fMNIST = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "# for i in mymnist_trainset.targets:\n",
    "#     num_of_class_MNIST[i] += 1\n",
    "for i in myfmnist_trainset.targets:\n",
    "    num_of_class_fMNIST[i] += 1\n",
    "    \n",
    "# print(num_of_class_MNIST)\n",
    "print(num_of_class_fMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ecf88-2767-4623-8ea0-98b1fc5c512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(14,5))\n",
    "# plt.subplot(1, 2, 1)  \n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.plot(valid_loss_history,label=\"val\")\n",
    "# plt.plot(train_loss_history,label=\"train\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2) \n",
    "# plt.title(\"Training and Validation Acc\")\n",
    "# plt.plot(valid_acc_history,label=\"val\")\n",
    "# plt.plot(train_acc_history,label=\"train\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Acc\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e8fc6-88fb-4dde-a3a5-eb6e455d2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './myfmnist_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# net = Net()\n",
    "# net.load_state_dict(torch.load(PATH))\n",
    "# net.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
